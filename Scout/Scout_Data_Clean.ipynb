{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Authors: Jason Belisario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aR_pOrpqLMSn"
      },
      "source": [
        "# Imports\n",
        "Finish Data cleaning post then watch pandas crash course vid and get on  colab to start with cleaning data sets\n",
        "\n",
        "1. Access websites and download whatever csv is there & save to GCS bucket\n",
        "\n",
        "2. Use Trifacta DataPrep to clean\n",
        "\n",
        "3. Go through DataPrep workflow to look at datasets and whatever cleaning they might need. Save the cleaned dataset in GCS\n",
        "\n",
        "* Web -> DataFrame to check csv works \n",
        "* DataFrame -> CSV \n",
        "* CSV -> pushed to GCS (using GCS Client Library)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VNCx3ICqEZCX"
      },
      "outputs": [],
      "source": [
        "# On colab you have to run every session\n",
        "#!pip install  datapackage   \n",
        "!pip install fredapi\n",
        "!pip install datapungi_fed "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "sAcLBD4TFgwv",
        "outputId": "abc21e0c-1b84-4da5-c881-2332f7fe7ee1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "#from datapackage import Package, Resource\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import storage \n",
        "import os \n",
        "from fredapi import Fred\n",
        "import requests\n",
        "import datapungi_fed as geoFred #use reg fred api for series & this one for geoFred only \n",
        "import datetime\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r''\n",
        "\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket('scout-storage')\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZW7h39I2sqsH"
      },
      "source": [
        "# Datahub.io Datasets\n",
        "**Package -> Resource -> Generator -> List -> DataFrame -> csv in GCS**  \n",
        "\n",
        "Won't use datahub but keep in case you find any interesting sets on that site "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WxwXpMWEMQDl"
      },
      "outputs": [],
      "source": [
        "# List resources available in datahub package:\n",
        "package = Package('https://datahub.io/core/finance-vix/datapackage.json')\n",
        "print(package.resource_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yM0ZM_Qd_ozY"
      },
      "outputs": [],
      "source": [
        "pre_trns_path = 'pre_trns/'\n",
        "\n",
        "datahub_sets = [{'GDP': 'https://datahub.io/core/gdp-us/datapackage.json'},\n",
        "                {'CPI': 'https://datahub.io/core/cpi-us/datapackage.json'},\n",
        "                {'Consumer_World_Inflation': 'https://datahub.io/core/inflation/datapackage.json'},\n",
        "                {'monthly_10_yr_treasury': 'https://datahub.io/core/bond-yields-us-10y/datapackage.json'},\n",
        "                {'Case-Shiller': 'https://datahub.io/core/house-prices-us/datapackage.json'},\n",
        "                {'household_income': 'https://datahub.io/core/household-income-us-historical/datapackage.json'}]\n",
        "\n",
        "for dataset in datahub_sets:\n",
        "  for dset_name, dset_link in dataset.items():\n",
        "\n",
        "    pkg = Package(dset_link)\n",
        "    resource_save = None\n",
        "    generator_to_list = []\n",
        "\n",
        "    for resource in pkg.resources:\n",
        "        if (resource.descriptor['datahub']['type'] == 'derived/csv' and \n",
        "            resource.descriptor['name'] == 'quarter_csv' or 'cpiai_csv' or\n",
        "            'inflation-consumer_csv' or 'monthly_csv' or 'cities_csv' or\n",
        "            'household-income-us-historical_csv') :\n",
        "          #print(resource.read(keyed = True)) \n",
        "          resource_save = resource\n",
        "\n",
        "    # iter() turns Resource object to Generator \n",
        "    resource_save = resource_save.iter(keyed = True)  \n",
        "\n",
        "    for row in resource_save:\n",
        "      generator_to_list.append(row)\n",
        "\n",
        "    df = pd.DataFrame(generator_to_list)\n",
        "    #print(df)\n",
        "\n",
        "    #When file is saved to bucket with same name, it just overwrites.\n",
        "    bucket.blob(pre_trns_path + dset_name +'.csv').upload_from_string(df.to_csv(), 'text/csv')\n",
        "\n",
        "    print('{}.csv uploaded to GCS within {}'.format(dset_name,pre_trns_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p2DabOegaTF9"
      },
      "source": [
        "# Fred.stlouisfed.org Datasets\n",
        "**FRED -> Series -> DataFrame -> csv in GCS**\n",
        "\n",
        "> **Monthly**\n",
        "* Consumer Price Index for All Urban Consumers: All Items in U.S. City Average (CPIAUCSL) - Monthly  \n",
        "* CBOE 10-Year Treasury Note Volatility Futures (VXTYN) -- frequency = 'm' (original set is daily)  \n",
        "* 10-Year Treasury Constant Maturity Rate (DGS10) -- frequency = 'm' (original set is daily)  \n",
        "* 30-Year Treasury Constant Maturity Rate (DGS30) -- frequency = 'm' (original set is daily)  \n",
        "* 15-Year Fixed Rate Mortgage Average in the United States (MORTGAGE15US) -- frequency = 'm' (original set is weekly)  \n",
        "* 30-Year Fixed Rate Mortgage Average in the United States (MORTGAGE30US) -- frequency = 'm' (original set is weekly)  \n",
        "* S&P/Case-Shiller U.S. National Home Price Index (CSUSHPINSA) - Monthly  \n",
        "* CBOE Volatility Index: VIX (VIXCLS) -- frequency = 'm' (original set is daily)   \n",
        "* 3-Month London Interbank Offered Rate (LIBOR), based on U.S. Dollar (USD3MTD156N) -- frequency = 'm' (original set is daily)  \n",
        "* 1-Month London Interbank Offered Rate (LIBOR), based on U.S. Dollar (USD1MTD156N) -- frequency = 'm' (original set is daily)  \n",
        "* 12-Month London Interbank Offered Rate (LIBOR), based on U.S. Dollar (USD12MD156N) -- frequency = 'm' (original set is daily)  \n",
        "* University of Michigan: Consumer Sentiment (UMCSENT) - Monthly  \n",
        "* Housing Starts: Total: New Privately Owned Housing Units Started (HOUST) - Monthly  \n",
        "* Commercial Bank Interest Rate on Credit Card Plans (TERMCBCCALLNS) - Monthly\n",
        "* Personal Saving Rate (PSAVERT) - Monthly\n",
        "* Unemployment Rate (UNRATE) - Monthly\n",
        "* Continued Claims (Insured Unemployment) (CCSA) -- frequency = 'm' (original set is weekly)\n",
        "* Total Vehicle Sales (TOTALSA) - Monthly - Seasonally Adjusted Annual Rate\n",
        "* Advance Retail Sales: Retail (Excluding Food Services) (RSXFS) - Monthly - Seasonally Adjusted\n",
        "\n",
        "Unemployment by sector. by % and thousands of persons  (Monthly)\n",
        "https://fred.stlouisfed.org/release/tables?rid=50&eid=4635#snid=4686\n",
        "\n",
        "> **Quarterly**\n",
        "* Real Gross Domestic Product (GDPC1) - Quarterly\n",
        "* Mutual Funds and Exchange-Traded Funds; Corporate Equities; Asset, Flow (BOGZ1FA483064105Q) - Quarterly\n",
        "* Mutual Funds and Exchange-Traded Funds; Long-Term Debt Securities; Asset, Flow (BOGZ1FA484022605Q) - Quarterly\n",
        "* New Privately Owned Housing Starts in the United States by Number of Units in Building, 20 or More Units (HOUSTDTA20UMQ) - Quarterly\n",
        "* Corporate Profits After Tax (without IVA and CCAdj) (CP) - Quarterly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6Z-h8Zv8ehAg"
      },
      "outputs": [],
      "source": [
        "FRED_API_KEY = ''\n",
        "fred = Fred(api_key= FRED_API_KEY)\n",
        "\n",
        "# trns = transformation \n",
        "pre_trns_path = 'FRED_pre_trns/'\n",
        "\n",
        "FRED_monthly = [{'Consumer Price Index for All Urban Consumers': 'CPIAUCSL'}, {'CBOE 10-Year Treasury Note Volatility Futures': 'VXTYN'},\n",
        "                {'10-Year Treasury Constant Maturity Rate': 'DGS10'}, {'30-Year Treasury Constant Maturity Rate': 'DGS30'},\n",
        "                {'15-Year Fixed Rate Mortgage Average': 'MORTGAGE15US'}, {'30-Year Fixed Rate Mortgage Average': 'MORTGAGE30US'},\n",
        "                {'S&P/Case-Shiller U.S. National Home Price Index': 'CSUSHPINSA'}, {'CBOE Volatility Index: VIX': 'VIXCLS'},\n",
        "                {'3-Month LIBOR, based on U.S. Dollar': 'USD3MTD156N'}, {'1-Month LIBOR, based on U.S. Dollar': 'USD1MTD156N'},\n",
        "                {'12-Month LIBOR, based on U.S. Dollar': 'USD12MD156N'}, {'University of Michigan: Consumer Sentiment': 'UMCSENT'},\n",
        "                {'Housing Starts: Total: New Privately Owned Housing': 'HOUST'}, {'Commercial Bank Interest Rate on Credit Card Plans': 'TERMCBCCALLNS'},\n",
        "                {'Personal Saving Rate': 'PSAVERT'}, {'Unemployment Rate': 'UNRATE'}, {'Continued Claims': 'CCSA'},\n",
        "                {'Total Vehicle Sales': 'TOTALSA'}, {'Retail Sales': 'RSXFS'}]\n",
        "\n",
        "# Main monthly, quarterly, and yearly dataframes.  \n",
        "\n",
        "monthly_join = pd.DataFrame(index = pd.date_range('2010-01-01', datetime.date.today(), freq='MS'))\n",
        "\n",
        "for dataset in FRED_monthly:\n",
        "  for dset_name, dset_id in dataset.items():\n",
        "    monthly_fred_data = fred.get_series(dset_id, observation_start= '2010-01-01', frequency = 'm').rename(dset_name)\n",
        "  monthly_join = monthly_join.join(monthly_fred_data).interpolate(method = 'time', limit_area = 'inside').fillna(value = 0) \n",
        "  \n",
        "#GCS functions to push df to cloud as csv\n",
        "# bucket.blob(pre_trns_path + 'Monthly_FRED.csv').upload_from_string(monthly_join.to_csv(), 'text/csv')\n",
        "\n",
        "# print('Monthly_FRED.csv uploaded to GCS within {}'.format(pre_trns_path))\n",
        "\n",
        "#pd.DataFrame(monthly_join)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TI4TL5neCFX_"
      },
      "outputs": [],
      "source": [
        "# FRED Quarterly \n",
        "FRED_quarterly = [{'Real Gross Domestic Product': 'GDPC1'},\n",
        "                  {'Mutual Funds & ETFs; Corporate Equities; Asset, Flow': 'BOGZ1FA483064105Q'},\n",
        "                  {'Mutual Funds & ETFs; Long-Term Debt Securities; Asset, Flow': 'BOGZ1FA484022605Q'},\n",
        "                  {'New Privately Owned Housing Starts (20+ units)': 'HOUSTDTA20UMQ'},\n",
        "                  {'Corporate Profits After Tax ': 'CP'}]\n",
        "\n",
        "quarterly_join = pd.DataFrame(index = pd.date_range('2010-01-01', datetime.date.today(), freq='MS'))\n",
        "\n",
        "for dataset in FRED_quarterly:\n",
        "  for dset_name, dset_id in dataset.items():\n",
        "    quarterly_fred_data = fred.get_series(dset_id, observation_start= '2010-01-01').rename(dset_name)\n",
        "  quarterly_join = quarterly_join.join(quarterly_fred_data).interpolate(method = 'time', limit_area = 'inside').fillna(value = 0)\n",
        "\n",
        "#GCS functions to push df to cloud as csv\n",
        "# bucket.blob(pre_trns_path + 'Quarterly_FRED.csv').upload_from_string(quarterly_join.to_csv(), 'text/csv')\n",
        "\n",
        "# print('Quarterly_FRED.csv uploaded to GCS within {}'.format(pre_trns_path))\n",
        "\n",
        "#pd.DataFrame(quarterly_join).head(n = 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "upHpTSoJl6qA"
      },
      "source": [
        "#Geo FRED\n",
        "Only need to have one of the series in the group to get it to search for all counties start_date = 2010-01-01 for now\n",
        "\n",
        "**Annual**\n",
        "* msa gdp - Annual - usable ID: NGMP29820\n",
        "\n",
        "* Estimate of Median Household Income - Annual - usable ID: MHICA06037A052NCEN\n",
        "* 2018 Bachelor's Degree or Higher (5-year estimate) by County - Annual - usable ID: HC01ESTVC1732017 (only up to 2018, forecast 2019 value by %change from 2017-2018)\n",
        "* 2018 High School Graduate or Higher (5-year estimate) by County - Annual - usable ID: HC01ESTVC1606071 \"\" \n",
        "* 2018 Per Capita Personal Income by County (dollars) - Annual - usable ID: PCPI06071\n",
        "* 2018 Personal Income by County (thousands of dollars) - Annual - usable ID: PI06071\n",
        "* 2019 Resident Population by County (change, thousands of persons) - Annual - usable ID: CASANB1POP\n",
        "* Combined Violent and Property Crime Incidents Known to Law Enforcement (Known Incidents) - Annual - usable ID: FBITC012086\n",
        "\n",
        "**Monthly**\n",
        "* 2020 March Unemployment Rate by County (percent) - Monthly - usable ID: CASANB1URN\n",
        "* 2020 March Civilian Labor Force by County (persons) - Monthly - usable ID: CASANB1LFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7tub94DyCn9L"
      },
      "outputs": [],
      "source": [
        "geo_pre_trns_path = 'FRED_pre_trns/geo/'\n",
        "geo_data = geoFred.data(FRED_API_KEY)\n",
        "\n",
        "geoFRED_county_year = [{'Estimate of Median Household Income ($)': 'MHICA06037A052NCEN'}, {\"Bachelor's Degree or Higher (%)\": 'HC01ESTVC1732017'},\n",
        "                       {'High School Graduate or Higher (%)': 'HC01ESTVC1606071'}, {'Per Capita Personal Income ($)': 'PCPI06071'},\n",
        "                       {'Personal Income (1000 * $)': 'PI06071'},{'Resident Population (thousands of persons)': 'CASANB1POP'},\n",
        "                       {'Violent and Property Crime Incidents': 'FBITC012086'}]\n",
        "\n",
        "# geo_year_join = pd.DataFrame(index = pd.date_range('2010-01-01', datetime.date.today(), freq='MS'))\n",
        "\n",
        "# for dataset in geoFRED_county_year:\n",
        "#   for dset_name, dset_id in dataset.items():\n",
        "#     yearly_geoFRED_county = geo_data.geo(series_id = dset_id, start_date = '2010-01-01').drop(columns=['series_id','code']).rename(columns = {'region': dset_name, '_date': 'date'}).dropna()\n",
        "#     # fix stacked data    \n",
        "#     yearly_geoFRED_county = yearly_geoFRED_county.pivot(index = 'date', columns = dset_name, values = 'value').add_suffix('_' + dset_name)\n",
        "\n",
        "#     # change columns from object types to numeric types\n",
        "#     for col in yearly_geoFRED_county:\n",
        "#       yearly_geoFRED_county[col] = pd.to_numeric(yearly_geoFRED_county[col], errors='coerce', downcast = 'integer')\n",
        "\n",
        "#     geo_year_join = geo_year_join.join(yearly_geoFRED_county).interpolate(method = 'time', limit_area = 'inside').fillna(value = 0)\n",
        "    \n",
        "\n",
        "#bucket.blob(geo_pre_trns_path + dset_name +'.csv').upload_from_string(yearly_geoFRED_county.to_csv(), 'text/csv')\n",
        "# print('{}.csv uploaded to GCS within {}'.format(dset_name,geo_pre_trns_path))\n",
        "\n",
        "#geo_year_join\n",
        "#pickle to save the bytestream so don't have to run function multiple times to see the same dataframe\n",
        "# geo_year_pickle = geo_year_join.to_pickle('/content/drive/My Drive/Saf Cap/Scout/Pandas_Pickles/geo_year_join.pickle')\n",
        "geo_year_pickle2 = pd.read_pickle('/content/drive/My Drive/Saf Cap/Scout/Pandas_Pickles/geo_year_join.pickle')\n",
        "\n",
        "\n",
        "pd.DataFrame(geo_year_pickle2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tIlDcWvb8tpp"
      },
      "outputs": [],
      "source": [
        "geo_pre_trns_path = 'FRED_pre_trns/geo/'\n",
        "geoFRED_county_month = [{'Unemployment Rate (%)': 'CASANB1URN'}, {\"Civilian Labor Force (persons)\": 'CASANB1LFN'}]\n",
        "\n",
        "# geo_month_join = pd.DataFrame(index = pd.date_range('2010-01-01', datetime.date.today(), freq='MS'))\n",
        "\n",
        "# for dataset in geoFRED_county_month:\n",
        "#   for dset_name, dset_id in dataset.items():\n",
        "#     monthly_geoFRED_county = geo_data.geo(series_id = dset_id, start_date = '2010-01-01').drop(columns=['series_id','code']).rename(columns = {'region': dset_name, '_date': 'date'}).dropna()\n",
        "#     # fix stacked data    \n",
        "#     monthly_geoFRED_county = monthly_geoFRED_county.pivot(index = 'date', columns = dset_name, values = 'value').add_suffix('_' + dset_name)\n",
        "\n",
        "#     # change columns from object types to numeric types\n",
        "#     for col in monthly_geoFRED_county:\n",
        "#       monthly_geoFRED_county[col] = pd.to_numeric(monthly_geoFRED_county[col], errors='coerce', downcast = 'integer')\n",
        "\n",
        "#     geo_month_join = geo_month_join.join(monthly_geoFRED_county).interpolate(method = 'time', limit_area = 'inside').fillna(value = 0)\n",
        "    \n",
        "\n",
        "# bucket.blob(geo_pre_trns_path + dset_name +'.csv').upload_from_string(monthly_geoFRED_county.to_csv(), 'text/csv')\n",
        "# print('{}.csv uploaded to GCS within {}'.format(dset_name,geo_pre_trns_path))\n",
        "\n",
        "#geo_month_join\n",
        "\n",
        "# geo_month_pickle = geo_month_join.to_pickle('/content/drive/My Drive/Saf Cap/Scout/Pandas_Pickles/geo_month_join.pickle')\n",
        "geo_month_pickle2 = pd.read_pickle('/content/drive/My Drive/Saf Cap/Scout/Pandas_Pickles/geo_month_join.pickle')\n",
        "\n",
        "# pd.DataFrame(geo_month_pickle2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VkJ7kNNw9oCe"
      },
      "outputs": [],
      "source": [
        "# MSA GDP \n",
        "geo_pre_trns_path = 'FRED_pre_trns/geo/'\n",
        "geoFRED_msa_year = [{'MSA GDP ($)': 'NGMP29820'}]\n",
        "\n",
        "# msa_annual_join = pd.DataFrame(index = pd.date_range('2010-01-01', datetime.date.today(), freq='MS'))\n",
        "\n",
        "# for dataset in geoFRED_msa_year:\n",
        "#   for dset_name, dset_id in dataset.items():\n",
        "#     yearly_geoFRED_msa = geo_data.geo(series_id = dset_id, start_date = '2010-01-01').drop(columns=['series_id','code']).rename(columns = {'region': dset_name, '_date': 'date'}).dropna()\n",
        "#     # fix stacked data    \n",
        "#     yearly_geoFRED_msa = yearly_geoFRED_msa.pivot(index = 'date', columns = dset_name, values = 'value').add_suffix('_' + dset_name)\n",
        "    \n",
        "#     # change columns from object types to numeric types\n",
        "#     for col in yearly_geoFRED_msa:\n",
        "#       yearly_geoFRED_msa[col] = pd.to_numeric(yearly_geoFRED_msa[col], errors='coerce', downcast = 'integer')\n",
        "\n",
        "#     msa_annual_join = msa_annual_join.join(yearly_geoFRED_msa).interpolate(method = 'time', limit_area = 'inside').fillna(value = 0)\n",
        "    \n",
        "\n",
        "# # bucket.blob(geo_pre_trns_path + dset_name +'.csv').upload_from_string(yearly_geoFRED_msa.to_csv(), 'text/csv')\n",
        "# # print('{}.csv uploaded to GCS within {}'.format(dset_name,geo_pre_trns_path))\n",
        "\n",
        "# #msa_annual_join\n",
        "\n",
        "# msa_annual_pickle = msa_annual_join.to_pickle('/content/drive/My Drive/Saf Cap/Scout/Pandas_Pickles/msa_annual_join.pickle')\n",
        "msa_annual_pickle2 = pd.read_pickle('/content/drive/My Drive/Saf Cap/Scout/Pandas_Pickles/msa_annual_join.pickle')\n",
        "\n",
        "# pd.DataFrame(msa_annual_pickle2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IKJDBQR48j4Z"
      },
      "source": [
        "#Joining FRED and GeoFRED data by date index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "phsYOq-w8jXw",
        "outputId": "b1b21980-101c-44e7-853e-ff4797827f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_geo.csv uploaded to GCS within FRED_pre_trns/\n"
          ]
        }
      ],
      "source": [
        "pre_trns_path = 'FRED_pre_trns/'\n",
        "master_join = pd.DataFrame(index = pd.date_range('2010-01-01', datetime.date.today(), freq='MS'))\n",
        "\n",
        "#df_list = [monthly_join, quarterly_join, geo_year_join, geo_month_join, msa_annual_join]\n",
        "#df_list = [monthly_join, quarterly_join]\n",
        "\n",
        "df_list = [geo_year_join]\n",
        "for df in df_list: \n",
        "  master_join = master_join.join(df, how = 'outer')\n",
        "\n",
        "master_join = master_join\n",
        "\n",
        "master_join\n",
        "\n",
        "file_name = 'test_geo.csv'\n",
        "bucket.blob(pre_trns_path + file_name).upload_from_string(master_join.to_csv(), 'text/csv')\n",
        "print('{} uploaded to GCS within {}'.format(file_name, pre_trns_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LwnKgwsrpicn"
      },
      "source": [
        "# Other economic data sources\n",
        "\n",
        "Mainly Federal Reserve Economic Indicators\n",
        "  1. Consumer Credit \n",
        "  2. Industrial Production and Capacity Utilization\n",
        "  3. Real Estate Owned (REO) \n",
        "\n",
        "Housing Credit Availability Index (HCAI) - https://www.urban.org/sites/default/files/hcai2019q4.xlsx  \n",
        "\n",
        "The HCAI measures the percentage of owner-occupied home purchase loans that are likely to defaultâ€”that is, go unpaid for more than 90 days past their due date.  \n",
        "\n",
        "A lower HCAI indicates that lenders are unwilling to \n",
        "tolerate defaults and are imposing tighter lending standards, making it harder to get a loan.  \n",
        "\n",
        "A higher HCAI indicates that lenders are willing to tolerate defaults and are taking more risks, making it easier to get a loan.\n",
        "\n",
        "* Consumer Credit Index (from OECD) - have to delete any sources that aren't USA location. Might just not use b/c already have UMich one \n",
        "* Real Estate Owned (REO) - i.e. Total real estate loans owned and securitized by finance companies\n",
        "* Consumer Credit Outstanding (CCO)\n",
        "* Industrial Production: Market and Industry Groups (IP)\n",
        "* Capacity Utilization (CU)\n",
        "\n",
        "**Link -> Dataframe -> GCS as csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n5aJTTOJpyDD"
      },
      "outputs": [],
      "source": [
        "econ_pre_trns_path = 'econ_pre_trns/'\n",
        "\n",
        "# read_excel\n",
        "HCAI_url = [{'Housing Credit Availability Index': 'https://www.urban.org/sites/default/files/hcai2019q4.xlsx'}] \n",
        "\n",
        "HCAI_join = pd.DataFrame(index = pd.date_range('2010-01-01', datetime.date.today(), freq='MS'))\n",
        "def create_quarters(row):\n",
        "   if row['Quarter'] == 1 :\n",
        "      return row['Year'] + '-01-01'\n",
        "   elif row['Quarter'] == 2 :\n",
        "      return row['Year'] + '-04-01'\n",
        "   elif row['Quarter'] == 3 :\n",
        "      return row['Year'] + '-07-01'\n",
        "   elif row['Quarter'] == 4 :\n",
        "      return row['Year'] + '-10-01'\n",
        "\n",
        "for dataset in HCAI_url:\n",
        "  for dset_name, dset_url in dataset.items():\n",
        "    \n",
        "    haci_df = pd.read_excel(dset_url)\n",
        "    haci_df.columns = haci_df.iloc[2] #set column names to a specific row \n",
        "    haci_df = haci_df.drop(index = [0,1,2], columns= 'highlight')\n",
        "\n",
        "    # Create dates out of Year and Quarter num original columns \n",
        "    haci_df['Year'] = haci_df['Year'].apply(str)\n",
        "    haci_df['Date'] = haci_df.apply(lambda row: create_quarters(row), axis=1)\n",
        "    haci_df['Date'] = pd.to_datetime(haci_df['Date'])\n",
        "\n",
        "    haci_df = haci_df.drop(columns= ['Year', 'Quarter']).set_index('Date').rename(columns = {'Total\\nRisk': 'Total Risk', 'Borrower\\nRisk': 'Borrower Risk', 'Product\\nRisk': 'Product Risk'})\n",
        "    HCAI_join = HCAI_join.join(haci_df, how = 'inner') # later maybe join outer to monthly df which allows for easier join to other monthly datasets if necessary \n",
        "\n",
        "HCAI_join\n",
        "# bucket.blob(econ_pre_trns_path + dset_name +'.csv').upload_from_string(HCAI_join.to_csv(), 'text/csv')\n",
        "# print('{}.csv uploaded to GCS within {}'.format(dset_name,econ_pre_trns_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ov1Jz9yLp1os"
      },
      "outputs": [],
      "source": [
        "# read_csv\n",
        "econ_pre_trns_path = 'econ_pre_trns/'\n",
        "fed_file = 'joined_fed.csv'\n",
        "Fed_urls = [{'REO_FED': 'https://www.federalreserve.gov/datadownload/Output.aspx?rel=G20&series=382ce44f124afc993f2b566c6c2cd4ef&lastobs=&from=&to=&filetype=csv&label=include&layout=seriescolumn&type=package'},\n",
        "            {'Consumer Credit Outstanding_FED':'https://www.federalreserve.gov/datadownload/Output.aspx?rel=G19&series=47b3133fcba3957706678b2a55cb5a97&lastobs=&from=&to=&filetype=csv&label=include&layout=seriescolumn&type=package'},\n",
        "            {'Industrial Production_FED':'https://www.federalreserve.gov/datadownload/Output.aspx?rel=G17&series=5d88c03b0036f0334d78f6bafefc5101&lastobs=&from=&to=&filetype=csv&label=include&layout=seriesrow&type=package'},\n",
        "            {'Capacity Utilization_FED':'https://www.federalreserve.gov/datadownload/Output.aspx?rel=G17&series=f213ee970e317c0f3cc80e775d35b8e5&lastobs=&from=&to=&filetype=csv&label=include&layout=seriesrow&type=package'}]\n",
        "\n",
        "#Fed_urls = [{'REO_FED': 'https://www.federalreserve.gov/datadownload/Output.aspx?rel=G20&series=382ce44f124afc993f2b566c6c2cd4ef&lastobs=&from=&to=&filetype=csv&label=include&layout=seriescolumn&type=package'}]\n",
        "\n",
        "fed_join = pd.DataFrame(index = pd.date_range('2010-01-01', datetime.date.today(), freq='MS'))\n",
        "for dataset in Fed_urls:\n",
        "  for dset_name, dset_url in dataset.items():\n",
        "    if dset_name == 'Capacity Utilization_FED' or dset_name == 'Industrial Production_FED':\n",
        "      fed_df = pd.read_csv(dset_url).drop(columns = ['Unit:', 'Multiplier:', 'Currency:', 'Unique Identifier:', 'Series Name:']).rename(columns = {'Descriptions:': 'Time'}).set_index('Time').T\n",
        "      fed_df.index = fed_df.index + '-01'\n",
        "      fed_df.index = pd.to_datetime(fed_df.index)\n",
        "    else: \n",
        "      fed_df = pd.read_csv(dset_url).drop(index = [0,1,2,3,4]).rename(columns = {'Series Description': 'Time'}).set_index('Time') \n",
        "      fed_df.index = fed_df.index + '-01'\n",
        "      fed_df.index = pd.to_datetime(fed_df.index)\n",
        "    \n",
        "    fed_join = fed_join.join(fed_df, how = 'inner').replace('ND', np.nan).apply(pd.to_numeric)  \n",
        "\n",
        "fed_join\n",
        "#bucket.blob(econ_pre_trns_path + fed_file).upload_from_string(fed_join.to_csv(), 'text/csv')\n",
        "#print('{} uploaded to GCS within {}'.format(fed_file,econ_pre_trns_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "d0yqYi4bp4Jx"
      },
      "outputs": [],
      "source": [
        "CCI_url = [{'CCI': 'https://stats.oecd.org/sdmx-json/data/DP_LIVE/.CCI.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en'}]\n",
        "\n",
        "for dataset in CCI_url:\n",
        "  for dset_name, dset_url in dataset.items():\n",
        "    cci_df = pd.read_csv(dset_url)\n",
        "\n",
        "cci_df\n",
        "\n",
        "# bucket.blob(econ_pre_trns_path + dset_name +'.csv').upload_from_string(cci_df.to_csv(), 'text/csv')\n",
        "# print('{}.csv uploaded to GCS within {}'.format(dset_name,econ_pre_trns_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "I5RpeI2c7si9",
        "outputId": "1a08cbbb-2bc0-4978-a094-555ac8e3eaa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pensions.csv uploaded to GCS within econ_pre_trns/\n"
          ]
        }
      ],
      "source": [
        "# send request \n",
        "econ_pre_trns_path = 'econ_pre_trns/'\n",
        "pensions_file = 'pensions.csv'\n",
        "pensions_url = [{'Public Pensions': 'http://publicplansdata.org/api/?q=QVariables&variables=PlanName,fy,EQTotal_Actl,PETotal_Actl,HFTotal_Actl,COMDTotal_Actl,RETotal_Actl,AltMiscTotal_Actl,CashTotal_Actl,OtherTotal_Actl,EQTotal_Trgt,FITotal_Trgt,PETotal_Trgt,HFTotal_Trgt,COMDTotal_Trgt,RETotal_Trgt,AltMiscTotal_Trgt,CashTotal_Trgt,OtherTotal_Trgt&filterfystart=2001&filterfyend=2020&format=json'}]\n",
        "\n",
        "for dataset in pensions_url:\n",
        "  for dset_name, dset_url in dataset.items():\n",
        "    r = requests.get(url = dset_url)\n",
        "    data = r.json()\n",
        "    pensions_df = pd.DataFrame(data)\n",
        "    pensions_df = pensions_df.drop(columns = ['status', 'date', 'q', 'params', 'recordcount']).rename(columns = {'fy': 'Time'}).set_index('Time').dropna(how = 'all')\n",
        "    \n",
        "    pensions_df.index = pensions_df.index + '-01'\n",
        "    pensions_df.index = pd.to_datetime(pensions_df.index)\n",
        "\n",
        "    pensions_df = pensions_df.fillna(0)\n",
        "    #.add_suffix('_' + dset_name)\n",
        "    pensions_df = pensions_df.pivot(columns = 'PlanName',)\n",
        "\n",
        "#pensions_df\n",
        "bucket.blob(econ_pre_trns_path + pensions_file).upload_from_string(pensions_df.to_csv(), 'text/csv')\n",
        "print('{} uploaded to GCS within {}'.format(pensions_file,econ_pre_trns_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t9Qc-bF9qSgZ"
      },
      "source": [
        "# Zillow Real Estate Data\n",
        "\n",
        " \n",
        "\n",
        "**Zillow Home Value Index (ZHVI)** \n",
        "* ZHVI All Homes (SFR, Condo/Co-op) Time-Series ($) - City - http://files.zillowstatic.com/research/public/City/City_Zhvi_AllHomes.csv   \n",
        "\n",
        "* ZHVI All Homes (SFR, Condo/Co-op) Time-Series ($) - County - http://files.zillowstatic.com/research/public/County/County_Zhvi_AllHomes.csv\n",
        "\n",
        "* ZHVI All Homes (SFR, Condo/Co-op) Time-Series ($) - MSA - http://files.zillowstatic.com/research/public/Metro/Metro_Zhvi_AllHomes.csv\n",
        "\n",
        "* ZHVI Single-Family Homes Time Series - City - http://files.zillowstatic.com/research/public/City/City_Zhvi_SingleFamilyResidence.csv\n",
        "\n",
        "* ZHVI Single-Family Homes Time Series - County - http://files.zillowstatic.com/research/public/County/County_Zhvi_SingleFamilyResidence.csv\n",
        "\n",
        "* ZHVI Single-Family Homes Time Series - MSA - http://files.zillowstatic.com/research/public/Metro/Metro_Zhvi_SingleFamilyResidence.csv\n",
        "\n",
        "**Zillow Rent Index (ZRI)**\n",
        "* ZRI Time-Series: Multifamily, SFR, Condo/Co-op ($) - City - http://files.zillowstatic.com/research/public/City/City_Zri_AllHomesPlusMultifamily.csv\n",
        "\n",
        "* ZRI Time-Series: Multifamily, SFR, Condo/Co-op ($) - County - http://files.zillowstatic.com/research/public/County/County_Zri_AllHomesPlusMultifamily.csv\n",
        "\n",
        "* ZRI Time-Series: Multifamily, SFR, Condo/Co-op ($) - MSA - http://files.zillowstatic.com/research/public/Metro/Metro_Zri_AllHomesPlusMultifamily.csv\n",
        "\n",
        "* ZRI Time-Series: Multifamily ($) - City - http://files.zillowstatic.com/research/public/City/City_Zri_MultiFamilyResidenceRental.csv\n",
        "\n",
        "* ZRI Time-Series: Multifamily ($) - County - http://files.zillowstatic.com/research/public/County/County_Zri_MultiFamilyResidenceRental.csv\n",
        "\n",
        "* ZRI Time-Series: Multifamily ($) - MSA - http://files.zillowstatic.com/research/public/Metro/Metro_Zri_MultiFamilyResidenceRental.csv\n",
        "\n",
        "**Misc.**\n",
        "* Age of Inventory (Days) - MSA - http://files.zillowstatic.com/research/public/Metro/MedianAgeOfInventory_NSA_AllHomes_Metro.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7wKwawadRAiO"
      },
      "outputs": [],
      "source": [
        "\n",
        "zillow_pre_trns_path = 'Zillow_pre_trns/'\n",
        "\n",
        "zillow_sfh_condo_values = [{'SFR, Condo Values - City':'http://files.zillowstatic.com/research/public/City/City_Zhvi_AllHomes.csv'},\n",
        "                           {'SFR, Condo Values - County':'http://files.zillowstatic.com/research/public/County/County_Zhvi_AllHomes.csv'},\n",
        "                           {'SFR, Condo Values - MSA':'http://files.zillowstatic.com/research/public/Metro/Metro_Zhvi_AllHomes.csv'}]\n",
        "\n",
        "for dataset in zillow_sfh_condo_values:\n",
        "  for dset_name, dset_url in dataset.items():\n",
        "    zillow_sfh_condo_values_df = pd.read_csv(dset_url)\n",
        "\n",
        "    bucket.blob(zillow_pre_trns_path + dset_name +'.csv').upload_from_string(zillow_sfh_condo_values_df.to_csv(), 'text/csv')\n",
        "\n",
        "    print('{}.csv uploaded to GCS within {}'.format(dset_name,zillow_pre_trns_path))\n",
        "\n",
        "zillow_sfh_values = [{'Single-Family Homes Values - City': 'http://files.zillowstatic.com/research/public/City/City_Zhvi_SingleFamilyResidence.csv'},\n",
        "                     {'Single-Family Homes Values - County': 'http://files.zillowstatic.com/research/public/County/County_Zhvi_SingleFamilyResidence.csv'},\n",
        "                     {'Single-Family Homes Values - MSA': 'http://files.zillowstatic.com/research/public/Metro/Metro_Zhvi_SingleFamilyResidence.csv'}]\n",
        "\n",
        "for dataset in zillow_sfh_values:\n",
        "  for dset_name, dset_url in dataset.items():\n",
        "    zillow_sfh_values_df = pd.read_csv(dset_url)\n",
        "\n",
        "    bucket.blob(zillow_pre_trns_path + dset_name +'.csv').upload_from_string(zillow_sfh_values_df.to_csv(), 'text/csv')\n",
        "\n",
        "    print('{}.csv uploaded to GCS within {}'.format(dset_name,zillow_pre_trns_path))\n",
        "\n",
        "zillow_Multi_Condo_rent = [{'Multifamily, SFR, Condo Rent - City': 'http://files.zillowstatic.com/research/public/City/City_Zri_AllHomesPlusMultifamily.csv'},\n",
        "                           {'Multifamily, SFR, Condo Rent - County': 'http://files.zillowstatic.com/research/public/County/County_Zri_AllHomesPlusMultifamily.csv'},\n",
        "                           {'Multifamily, SFR, Condo Rent - MSA': 'http://files.zillowstatic.com/research/public/Metro/Metro_Zri_AllHomesPlusMultifamily.csv'}]   \n",
        "\n",
        "for dataset in zillow_Multi_Condo_rent:\n",
        "  for dset_name, dset_url in dataset.items():\n",
        "    zillow_Multi_Condo_rent_df = pd.read_csv(dset_url, encoding= \"ISO-8859-1\")\n",
        "\n",
        "    bucket.blob(zillow_pre_trns_path + dset_name +'.csv').upload_from_string(zillow_Multi_Condo_rent_df.to_csv(), 'text/csv')\n",
        "\n",
        "    print('{}.csv uploaded to GCS within {}'.format(dset_name,zillow_pre_trns_path))\n",
        "\n",
        "\n",
        "zillow_Multi_rent = [{'Multifamily Rent - City': 'http://files.zillowstatic.com/research/public/City/City_Zri_MultiFamilyResidenceRental.csv'},\n",
        "                     {'Multifamily Rent - County': 'http://files.zillowstatic.com/research/public/County/County_Zri_MultiFamilyResidenceRental.csv'},\n",
        "                     {'Multifamily Rent - MSA': 'http://files.zillowstatic.com/research/public/Metro/Metro_Zri_MultiFamilyResidenceRental.csv'}]  \n",
        "\n",
        "for dataset in zillow_Multi_rent:\n",
        "  for dset_name, dset_url in dataset.items():\n",
        "    zillow_Multi_rent_df = pd.read_csv(dset_url)\n",
        "\n",
        "    bucket.blob(zillow_pre_trns_path + dset_name +'.csv').upload_from_string(zillow_Multi_rent_df.to_csv(), 'text/csv')\n",
        "\n",
        "    print('{}.csv uploaded to GCS within {}'.format(dset_name,zillow_pre_trns_path))\n",
        "\n",
        "\n",
        "zillow_inventory_age = [{'Age of Inventory (Days) - MSA': 'http://files.zillowstatic.com/research/public/Metro/MedianAgeOfInventory_NSA_AllHomes_Metro.csv'}]\n",
        "\n",
        "for dataset in zillow_inventory_age:\n",
        "  for dset_name, dset_url in dataset.items():\n",
        "    zillow_inventory_age_df = pd.read_csv(dset_url)\n",
        "\n",
        "    bucket.blob(zillow_pre_trns_path + dset_name +'.csv').upload_from_string(zillow_inventory_age_df.to_csv(), 'text/csv')\n",
        "\n",
        "    print('{}.csv uploaded to GCS within {}'.format(dset_name,zillow_pre_trns_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3_DEO482pVqs"
      },
      "source": [
        "# Test block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eBUNZnNb8rBn"
      },
      "outputs": [],
      "source": [
        "capacity_util = 'https://www.federalreserve.gov/datadownload/Output.aspx?rel=G17&series=f213ee970e317c0f3cc80e775d35b8e5&lastobs=&from=&to=&filetype=csv&label=include&layout=seriesrow&type=package'\n",
        "\n",
        "df = pd.read_csv(capacity_util)\n",
        "#drop columns and transpose so date is index \n",
        "df = df.drop(columns= ['Unit:', 'Multiplier:', 'Currency:', 'Unique Identifier:', 'Series Name:']).set_index(['Descriptions:']).T\n",
        "\n",
        "#only use rows where timestamp index >= Jan 1, 2010\n",
        "master_join.loc[(master_join.index >= pd.Timestamp('2010-01-01'))]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2o2EcZO6rI_M"
      },
      "source": [
        "# match CBG's of a market to that market. Get the 130 ish markets that we will cover and match MSA's, CBG's, Counties, to our Yardi Markets\n",
        "\n",
        "Split up demographic data to the Yardi Markets. \n",
        "\n",
        "5/30 - all csv's merged from census data. Deleted any columns with margin of error data. Kept only estimate columns.  \n",
        "NaN's in the first batch of columns. Figure out later why this is. - *fixed with right join*   \n",
        "After merge. Index has .0 in it. \n",
        "otherwise all cols & rows seem to be there\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "FIPS code: 12 digits   \n",
        "1-2 digits = state  \n",
        "3-5 digits = county   \n",
        "6-12 digits don't matter (tract & block group)  \n",
        "\n",
        "\n",
        "I want to get the Census block group data and group it under its correct yardi market \n",
        "yardi market -> county name -> county ID \n",
        "\n",
        "Loop through list of yardi market county ID's (must be same as Census ID's), for every ID, create new dataframe that is comprised of all the CBG rows in county_id col with said yardi_county_id using Groupby.\n",
        "\n",
        "Write each of those dataframes to csv a folder for yardi_market_census_data in GCS. Should be around 130 csv's for all of the yardi markets. \n",
        "\n",
        "\n",
        "Census Block group -> County ID \n",
        "\n",
        "Should i get our list of markets and write an excel file with their corresponding counties? \n",
        "Then delete counties that don't match up to any of the counties that correspond to our markets? \n",
        " \n",
        "Dropped CBG file 22. Issue from source with Index not being correct and didn't want to copy index from other CBG file and assume those indices would be correct. Also the actual data didn't seem useful so doesn't seem like a real loss. (number of disabled persons on food stamps if i'm not mistaken) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wv8NYKezPrhz"
      },
      "outputs": [],
      "source": [
        "!pip install gcsfs\n",
        "!pip install partd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "lhSnn0JXt9NY",
        "outputId": "352905c8-a421-45c0-c292-c9b4851ba7c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "from io import BytesIO\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "import os \n",
        "import dask.dataframe as dd\n",
        "import gcsfs\n",
        "import partd\n",
        "from itertools import repeat\n",
        "import json\n",
        "from functools import reduce\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r''\n",
        "\n",
        "storage_client = storage.Client()\n",
        "project_id = ''\n",
        "token_file = ''\n",
        "fs = gcsfs.GCSFileSystem(project = project_id)\n",
        "gcs_bucket_name = 'scout-storage'\n",
        "file_name = '/census/cbg_b00.csv' # get first csv to create index \n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TJ3s9IJXb7eG"
      },
      "outputs": [],
      "source": [
        "# dataframe's that will be concatenated and merged written here & saved to pickles \n",
        "census_index = dd.read_csv('gs://'+ gcs_bucket_name + file_name, storage_options={'token': token_file, 'project': project_id})\n",
        "census_index = census_index.drop(columns = ['B00001e1', 'B00001m1', 'B00002e1', 'B00002m1'])\n",
        "census_index['census_block_group'] = census_index['census_block_group'].astype(int).astype(str)\n",
        "\n",
        "# parse state and county id's \n",
        "state_county_id = census_index.compute() \n",
        "state_county_id['state_id'] = state_county_id.apply(lambda row: row['census_block_group'][0:2] if len(row['census_block_group']) == 12 else row['census_block_group'][0], axis=1)\n",
        "state_county_id['county_id'] = state_county_id.apply(lambda row: row['census_block_group'][2:5] if len(row['census_block_group']) == 12 else row['census_block_group'][1:4], axis=1)\n",
        "state_county_id = state_county_id.set_index('census_block_group')\n",
        "\n",
        "census_index = census_index.set_index('census_block_group')\n",
        "census_index = census_index.compute() \n",
        "\n",
        "state_county_id.to_pickle('/content/drive/My Drive/Saf Cap/Scout/Pandas_Pickles/state_county_id.pickle')\n",
        "census_index.to_pickle('/content/drive/My Drive/Saf Cap/Scout/Pandas_Pickles/census_index.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AU9PEQWwqnTy"
      },
      "outputs": [],
      "source": [
        "def get_blob_list(**kwargs):\n",
        "  blobs = storage_client.list_blobs('scout-storage', prefix = 'census/cbg')\n",
        "  num_blobs = kwargs.get('num_blobs', 21)\n",
        "  i = 0\n",
        "  blob_list = []\n",
        "  for blob in blobs:  \n",
        "    if i < num_blobs: \n",
        "      blob_list.append(blob.name)\n",
        "    else: \n",
        "      break\n",
        "    i += 1\n",
        "    \n",
        "  return blob_list\n",
        "\n",
        "#return cbg files as pandas dataframes\n",
        "def cbg_files(blob_name):\n",
        "  project_id = 'sigma-cairn-275317'\n",
        "  token_file = '/content/drive/My Drive/0 - GCP Credentials/Scout-19edc12a6cf5.json'\n",
        "  gcs_bucket_name = 'scout-storage'\n",
        "\n",
        "  #print(blob_name)\n",
        "  df = dd.read_csv('gs://'+ gcs_bucket_name + '/' + blob_name, storage_options={'token': token_file, 'project': project_id}, assume_missing = True)\n",
        "  droplist = [col_name for col_name in df.columns if col_name[-2] == 'm' or col_name[-3] == 'm'] \n",
        "  df = df.drop(droplist, axis = 1)\n",
        "  df['census_block_group'] = df['census_block_group'].astype(int).astype(str)\n",
        "  df = df.set_index('census_block_group') # set index to be able to merge \n",
        "  return df.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yEKCTYhDrWj3"
      },
      "outputs": [],
      "source": [
        "#blobs with state & county id's appended and saved as pickles \n",
        "state_county_id_df = pd.read_pickle('/content/drive/My Drive/Saf Cap/Scout/Pandas_Pickles/state_county_id.pickle')\n",
        "\n",
        "census_blobs = get_blob_list(num_blobs = 21) # get census blob list\n",
        "print(census_blobs)\n",
        "\n",
        "# for blob_name in census_blobs:\n",
        "#   census_cbg_df = cbg_files(blob_name)\n",
        "#   census_cbg_df = pd.concat([census_cbg_df, state_county_id_df], axis = 1) # concat with state/county id's \n",
        "  \n",
        "#   # [7:-4] to get rid of the census/ & .csv file designation \n",
        "#   census_cbg_df.to_pickle('/content/drive/My Drive/Saf Cap/Scout/Pandas_Pickles/cbg_blob_list_ids/' + blob_name[7:-4] + '.pickle')\n",
        "#   print(blob_name[7:-4] + ' uploaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "I8OApAZf-eCt"
      },
      "outputs": [],
      "source": [
        "cbg_list = []\n",
        "for blob_name in census_blobs:\n",
        " \n",
        "   cbg_list.append(pd.read_pickle('/content/drive/My Drive/Saf Cap/Scout/Pandas_Pickles/cbg_blob_list_ids/' + blob_name[7:-4] + '.pickle'))\n",
        "\n",
        "print('Done')\n",
        "#print(cbg_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "b-MDSIClQtvX"
      },
      "outputs": [],
      "source": [
        "client = storage.Client()\n",
        "bucket = client.get_bucket('scout-storage')\n",
        "\n",
        "# Opening markets_dict JSON file \n",
        "with open(\"\") as json_file: \n",
        "    markets = json.load(json_file) \n",
        "\n",
        "x = 0\n",
        "county_df_list = []\n",
        "market_df_list = []\n",
        "\n",
        "def group_by_market(census_df_list, markets_dict):\n",
        "  for market in markets_dict:\n",
        "    # logic for cbg file work here. just run fxn one line return census_cbg_df\n",
        "    for cbg_df in census_df_list: # not the issue\n",
        "      for market_name, market_details in market.items():\n",
        "        try:\n",
        "          for x in range(4):\n",
        "\n",
        "            x += 1\n",
        "            x = str(x)\n",
        "            state_num = 'state_' + x # to loop through keys state_1 through state_4 if they exist \n",
        "            state_id = market_details[state_num]['state_id']\n",
        "            counter = 1\n",
        "\n",
        "            for county_name, county_id in market_details[state_num]['counties'].items():\n",
        "              county_id = county_id.zfill(3) # pad county id's with 0's if county_id < 3 characters. \n",
        "              concat_df = cbg_df.loc[(cbg_df['state_id'] == state_id) & (cbg_df['county_id'] == county_id)]\n",
        "              concat_df = concat_df.drop(columns= ['state_id','county_id'])\n",
        "              county_df_list.append(concat_df)\n",
        "\n",
        "        except:\n",
        "          pass\n",
        "        \n",
        "        for df in county_df_list:\n",
        "          df = df.reset_index(inplace=True)\n",
        "          \n",
        "        #returns concatenated df FOR THAT MARKET using 1 cbg file. should break out of loop and go to next item on CBG_list list\n",
        "        concat_result_df = pd.concat(county_df_list, axis = 1, join = 'inner') #problem in concat lines \n",
        "        concat_result_df = concat_result_df.loc[:,~concat_result_df.columns.duplicated()]\n",
        "        county_df_list.clear() # clear items in list as to not conc. all counties of each market\n",
        "        #for each market & each CBG file, Append that CBG parsed file to list \n",
        "      market_df_list.append(concat_result_df) \n",
        "  \n",
        "    market_result_df = pd.concat(market_df_list, axis = 1, join = 'inner')#verify_integrity = True\n",
        "    market_result_df = market_result_df.loc[:,~market_result_df.columns.duplicated()].rename(columns= {'index': 'census_block_group'})\n",
        "    market_df_list.clear()\n",
        "\n",
        "    print(market_result_df)\n",
        "    bucket.blob('yardi_market_census/' + market_name + '.csv').upload_from_string(market_result_df.to_csv(), 'text/csv')\n",
        "    print(market_name + '.csv uploaded')\n",
        "    \n",
        "\n",
        "group_by_market(cbg_list, markets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oVToI5ez_RGp"
      },
      "outputs": [],
      "source": [
        "# checking if all 220k rows from each cbg file are the same \n",
        "from itertools import cycle\n",
        "project_id = ''\n",
        "token_file = ''\n",
        "fs = gcsfs.GCSFileSystem(project=project_id)\n",
        "gcs_bucket_name = 'scout-storage'\n",
        "\n",
        "cycle_blobs = cycle(census_blobs)\n",
        "next_file = next(cycle_blobs) #start the iterating at file 00 \n",
        "\n",
        "for blob_name in census_blobs:\n",
        "  \n",
        "  #[7:-4] to get rid of the census/ & .csv file designation \n",
        "  \n",
        "  print(blob_name)\n",
        "  next_file = next(cycle_blobs)\n",
        "  df = pd.read_csv('gs://'+ gcs_bucket_name +'/' + blob_name)\n",
        "  df['census_block_group'] = df['census_block_group'].astype(int).astype(str) \n",
        "  df = df.loc[:, df.columns.intersection(['census_block_group'])]\n",
        "\n",
        "  next_df = pd.read_csv('gs://'+ gcs_bucket_name +'/' + next_file)# 01\n",
        "  print('next file: ' + next_file)\n",
        "  next_df['census_block_group'] = next_df['census_block_group'].astype(int).astype(str) \n",
        "  next_df = next_df.loc[:, next_df.columns.intersection(['census_block_group'])]\n",
        "\n",
        "  test = df.equals(next_df)\n",
        "  \n",
        "  print(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O-tKa20eAO3c"
      },
      "source": [
        "# REIS\n",
        "\n",
        "Clean and join REIS data\n",
        "\n",
        "For markets without a cap rate, add a column for cap rate and fill it with NaN's or 0's.\n",
        "\n",
        "\n",
        "Just do the regular fix and join to REIS and then later once you have all of them joined, iterate through all lists to get an average of the cap rate for each month. then use that to populate list of the ones with 0's in the rates. Do same with expense %. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ceWlv8ymAi1n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import re\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GOYf7RXZR1ZA"
      },
      "outputs": [],
      "source": [
        "#pd.set_option('display.max_rows', 300)\n",
        "REIS_test_path = ''\n",
        "trends_path = ''\n",
        "\n",
        "def transform_trends(path):\n",
        "  try: \n",
        "    trends_df = pd.read_csv(path)\n",
        "  except:\n",
        "    trends_df = pd.read_excel(path)\n",
        "\n",
        "  if 'Cap Rate' in trends_df:\n",
        "    try:\n",
        "      trends_df['Cap Rate'] = trends_df['Cap Rate'].replace({'â€“': 0})\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  trends_df['Year'] = pd.to_datetime(trends_df['Year'], format='%Y')\n",
        "  trends_df = trends_df.set_index('Year')\n",
        "  upsampled = trends_df.resample('MS').interpolate()\n",
        "  return upsampled\n",
        "\n",
        "transform_trends(trends_path)#.tail(n = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Vn2ZfMHP9o31"
      },
      "outputs": [],
      "source": [
        "#REIS Report \n",
        "pd.set_option('display.max_rows', 300)\n",
        "\n",
        "REIS_test_path = ''\n",
        "\n",
        "def transform_REIS(path):\n",
        "  try: \n",
        "    REIS_df = pd.read_csv(path)\n",
        "  except:\n",
        "    REIS_df = pd.read_excel(path)\n",
        "\n",
        "  REIS_df = REIS_df.loc[(REIS_df['Building Class'] == 'All')]\n",
        "  REIS_df = REIS_df[REIS_df['Period'].str.contains('Q')]\n",
        "  REIS_df['Period'] = REIS_df['Period'].replace({'Q1': '0101', 'Q2': '0401', 'Q3': '0701', 'Q4': '1001'})\n",
        "  REIS_df['Date'] = REIS_df['Period'] + REIS_df['Year'].astype(str)\n",
        "  REIS_df['Date'] = pd.to_datetime(REIS_df['Date'],format='%m%d%Y', dayfirst = True)\n",
        "  REIS_df = REIS_df.drop(columns= ['Sector', 'Metro Code', 'Building Class', 'Year', 'Period', 'State' ]).set_index('Date').resample('MS').interpolate()\n",
        "  REIS_df['Market Name'] = REIS_df['Market Name'].fillna(method = 'pad')\n",
        "  return REIS_df\n",
        "\n",
        "#transform_REIS(REIS_test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yy-zV9PGwhn6"
      },
      "outputs": [],
      "source": [
        "#RE parse for market name b/c only will use sum function if there are more than 1 files in a directory with REIS or trends name. \n",
        "#RE parse market name and make file name marketname_REIS_joined.csv \n",
        "#call sum function after both transform and join functions are called \n",
        "\n",
        "\n",
        "\n",
        "def sum_df_list(df_list):\n",
        "  return pd.concat(df_list, axis = 1).groupby(level = 0, axis = 1).sum()\n",
        "\n",
        "#sum_df_list(list_of_dfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MiEIztcvRF1w"
      },
      "outputs": [],
      "source": [
        "\n",
        "#full_join.to_csv(path_or_buf='/content/drive/My Drive/test.csv') \n",
        "\n",
        "def join_REIS_trends (REIS_df, trends_df):\n",
        "  REIS_trends_df = REIS_df.join(trends_df, how = 'left')\n",
        "  return REIS_trends_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LeWHEZf-HvHM"
      },
      "outputs": [],
      "source": [
        "#expense % and cap rate for all markets averaged out and joined\n",
        "test_dir = ''\n",
        "\n",
        "exp_and_cap_list = [] # list of all df's with caprate & expense \n",
        "for dirpath, dirnames, filenames in os.walk(test_dir):\n",
        "  trends_path_list = []\n",
        "  REIS_path_list = []\n",
        "  \n",
        "  trends_df_list = []\n",
        "  REIS_df_list = []\n",
        "\n",
        "  \n",
        "  for file in filenames:\n",
        "    \n",
        "    REIS_path = dirpath + '/' + file\n",
        "\n",
        "    file_name = re.compile(r'^(.+)\\/([^/]+)\\/([^/]+)').sub(r'\\3', REIS_path) # group 3 to get file name \n",
        "    market_name = re.compile(r'^(.+)\\/([^/]+)\\/([^/]+)').sub(r'\\2', REIS_path) # group 2 to get market name \n",
        "    file_save_name = market_name + '_market_data.csv'\n",
        "    save_path = os.path.join(dirpath, file_save_name)\n",
        "    #print(save_path)\n",
        "    if 'trends' in file_name: \n",
        "      trends_path_list.append(REIS_path) # to get list of paths to the trends files\n",
        "    elif 'REIS' in file_name:\n",
        "      REIS_path_list.append(REIS_path)\n",
        "\n",
        "  if len(REIS_path_list) > 1: # if you're in a market with more than 1 REIS csv, then there will also be > 1 trends file as well  \n",
        "    pass\n",
        "  else:\n",
        "    try: #to pass the list with nothing in them \n",
        "      REIS_df = transform_REIS(REIS_path_list[0])\n",
        "      trends_df = transform_trends(trends_path_list[0])\n",
        "      joined_df = join_REIS_trends(REIS_df, trends_df)\n",
        "      if 'Cap Rate' in joined_df.columns and 'Expenses %' in joined_df.columns:\n",
        "        exp_and_cap_list.append(joined_df)\n",
        "        print(market_name)\n",
        "        #print(joined_df.head())\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        " \n",
        "exp_and_cap_df = pd.concat(exp_and_cap_list, axis = 1)\n",
        "exp_and_cap_df = exp_and_cap_df.drop(exp_and_cap_df.columns.difference(['Cap Rate','Expenses %']), axis = 1).groupby(level = 0, axis = 1).mean()\n",
        "exp_and_cap_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Mq_m8Xl0ZQN2"
      },
      "outputs": [],
      "source": [
        "#pd.set_option('display.max_columns', 20)\n",
        "\n",
        "# to catch if market has more than 1 file with REIS | trends in it \n",
        "dir = ''\n",
        "\n",
        "# mkt_name_REIS_joined.csv = regex parse market path from REIS_path up to last '/' + regex parse market name + '.csv'\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(dir):\n",
        "  trends_path_list = []\n",
        "  REIS_path_list = []\n",
        "  \n",
        "  trends_df_list = []\n",
        "  REIS_df_list = []\n",
        "\n",
        "  #market_name = ''\n",
        "  for file in filenames:\n",
        "    \n",
        "    REIS_path = dirpath + '/' + file\n",
        "\n",
        "    file_name = re.compile(r'^(.+)\\/([^/]+)\\/([^/]+)').sub(r'\\3', REIS_path) # group 3 to get file name \n",
        "    market_name = re.compile(r'^(.+)\\/([^/]+)\\/([^/]+)').sub(r'\\2', REIS_path) # group 2 to get market name \n",
        "    file_save_name = market_name + '_market_data.csv'\n",
        "    save_path = os.path.join(dirpath, file_save_name)\n",
        "    #print(save_path)\n",
        "    if 'trends' in file_name: \n",
        "      trends_path_list.append(REIS_path) # to get list of paths to the trends files\n",
        "    elif 'REIS' in file_name:\n",
        "      REIS_path_list.append(REIS_path)\n",
        "\n",
        "  if len(REIS_path_list) > 1: # if you're in a market with more than 1 REIS csv, then there will also be > 1 trends file as well \n",
        "    # clean trends and REIS df's and append them to lists \n",
        "    for REIS_csv_path in REIS_path_list:\n",
        "      #print(REIS_csv_path)\n",
        "      REIS_df_list.append(transform_REIS(REIS_csv_path))\n",
        "\n",
        "    for trends_csv_path in trends_path_list:\n",
        "      trends_df_list.append(transform_trends(trends_csv_path))\n",
        "\n",
        "    REIS_sum_df = sum_df_list(REIS_df_list)\n",
        "    trends_sum_df = sum_df_list(trends_df_list)\n",
        "\n",
        "    sum_joined_df = join_REIS_trends(REIS_sum_df, trends_sum_df) \n",
        "    sum_joined_df['Market Name'] = market_name \n",
        "\n",
        "    if 'Cap Rate' not in sum_joined_df.columns and 'Expenses %' not in sum_joined_df.columns:\n",
        "      sum_joined_df = sum_joined_df.join(exp_and_cap_df, how = 'left')\n",
        "\n",
        "    sum_joined_df.to_csv(path_or_buf = save_path) \n",
        "\n",
        "    print(market_name)\n",
        "    print(sum_joined_df.head())\n",
        "  else:\n",
        "    #don't need to sum up df's. just clean and join \n",
        "    try: #to pass the list with nothing in them \n",
        "      REIS_df = transform_REIS(REIS_path_list[0])\n",
        "      trends_df = transform_trends(trends_path_list[0])\n",
        "    except:\n",
        "      pass\n",
        "    joined_df = join_REIS_trends(REIS_df, trends_df)\n",
        "\n",
        "    if 'Cap Rate' not in joined_df.columns and 'Expenses %' not in joined_df.columns:\n",
        "      joined_df = joined_df.join(exp_and_cap_df, how = 'left')\n",
        "\n",
        "    joined_df.to_csv(path_or_buf = save_path)\n",
        "\n",
        "    print(market_name)\n",
        "    print(joined_df.head()) "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ZW7h39I2sqsH",
        "p2DabOegaTF9",
        "upHpTSoJl6qA",
        "IKJDBQR48j4Z",
        "LwnKgwsrpicn",
        "t9Qc-bF9qSgZ",
        "3_DEO482pVqs",
        "2o2EcZO6rI_M"
      ],
      "machine_shape": "hm",
      "name": "Scout_Data_Clean.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
