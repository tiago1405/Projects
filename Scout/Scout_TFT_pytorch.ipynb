{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Authorship"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adapted from\n",
        "```\n",
        "Bryan Lim, Sercan Ö. Arık, Nicolas Loeff, Tomas Pfister,\n",
        "Temporal Fusion Transformers for interpretable multi-horizon time series forecasting,\n",
        "International Journal of Forecasting,\n",
        "Volume 37, Issue 4,\n",
        "2021,\n",
        "Pages 1748-1764,\n",
        "ISSN 0169-2070,\n",
        "https://doi.org/10.1016/j.ijforecast.2021.03.012.\n",
        "(https://www.sciencedirect.com/science/article/pii/S0169207021000637)\n",
        "```\n",
        "Specifically the 2019 version: https://arxiv.org/abs/1912.09363\n",
        "\n",
        "Using PyTorch Forecasting: https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html\n",
        "\n",
        "Further modified by: Tiago Zanaga Da Costa and Jason Belisario\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_gEMDjdrcsN"
      },
      "source": [
        "# Preliminary Setup \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu_BnDv24ypp",
        "outputId": "0c81b073-a112-4270-b165-c0c3a72e6ff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqV463FmvV_C"
      },
      "source": [
        "## Package installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdB4uDzontiv"
      },
      "outputs": [],
      "source": [
        "!pip3 install gitpython pyunpack wget patool plotly cufflinks --user\n",
        "#!pip install tensorflow==1.15 #final tf v1. v2 incompatible    don't need when using vm \n",
        "\n",
        "# Resets the IPython kernel to import the installed package.\n",
        "# import IPython\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG6Md79jcRTN"
      },
      "outputs": [],
      "source": [
        "!pip install gcsfs\n",
        "!pip install torch -f https://download.pytorch.org/whl/torch_stable.html.\n",
        "!pip install pytorch-forecasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4ZmIBQgDPl0"
      },
      "source": [
        "## Import Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eiyBTMob5ia"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# os.chdir(\"../../..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trc3hzgxb697"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "\n",
        "from pytorch_forecasting.metrics import PoissonLoss, QuantileLoss, SMAPE\n",
        "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc59MnmN0at-"
      },
      "source": [
        "## Code Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2jvryTGIwJR"
      },
      "outputs": [],
      "source": [
        "import shutil \n",
        "#shutil.rmtree('/content/scout_tft')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8yiZpWC0Nhu",
        "outputId": "b15401be-02e8-4756-f61d-afcbe7881325"
      },
      "outputs": [],
      "source": [
        "# Code Download \n",
        "import os \n",
        "from git import Repo\n",
        "\n",
        "username = \"\"\n",
        "password = \"\"\n",
        "git_url = '.../scout_tft.git'\n",
        "# to clone private repo\n",
        "remote = f\"https://{username}:{password}@github.com/.../scout_tft.git\"\n",
        "\n",
        "# Current working dir \n",
        "repo_dir = os.getcwd() + '/scout_tft'\n",
        "\n",
        "if not os.path.exists(repo_dir):\n",
        "  os.makedirs(repo_dir)\n",
        "  print (\"Creating {}...\".format(repo_dir)) \n",
        "\n",
        "# Clone pytorch tft githup repo\n",
        "if not os.listdir(repo_dir):\n",
        "  print (\"Cloning {} to {}...\".format(git_url, repo_dir)) \n",
        "  Repo.clone_from(remote, repo_dir, branch='master')\n",
        "\n",
        "# Set current working directory to /content/pytorch-tft-repo\n",
        "os.chdir(repo_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9qz4J2lJuiT",
        "outputId": "dd9dc9d2-b0a8-470a-e411-566ee90d01e2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/scout_tft/scout_tft'"
            ]
          },
          "execution_count": 28,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La9Fu4gV2fnH"
      },
      "source": [
        "## Data Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxUAJcqvOpj4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "import gcsfs\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-eIT_LKH4mn"
      },
      "outputs": [],
      "source": [
        "#os.chdir('.../tft-repo') # when you're using a VM \n",
        "# Download parameters \n",
        "# experiment_name = 'scout'\n",
        "# output_folder = os.path.join(os.getcwd(), 'outputs')\n",
        "\n",
        "# if not os.path.exists(output_folder):\n",
        "#   os.makedirs(output_folder)\n",
        "\n",
        "# data_folder = os.path.join(output_folder, 'data', experiment_name)\n",
        "# if not os.path.exists(data_folder):\n",
        "#   os.makedirs(data_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_hI7xdDMQZ3"
      },
      "outputs": [],
      "source": [
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r''\n",
        "#os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r''\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = ''\n",
        "\n",
        "my_bucket = 'scout-storage'\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket(my_bucket)\n",
        "\n",
        "project_id = ''\n",
        "token_file = ''\n",
        "fs = gcsfs.GCSFileSystem(project=project_id)\n",
        "\n",
        "markets_prefix = \"markets_final/\"\n",
        "market_name = \"market074.parquet\"\n",
        "# market_path = \"gs://\" + markets_prefix + market_name\n",
        "market_path = 'gs://scout-storage/markets_final' +  '/' + market_name\n",
        "markets_blobs = bucket.list_blobs(prefix = markets_prefix, delimiter = '/')\n",
        "markets_path_list = []\n",
        "\n",
        "for blob in markets_blobs:\n",
        "  #if '.csv' in blob.name:\n",
        "  blob_path = 'gs://' + my_bucket + '/' + blob.name  \n",
        "  markets_path_list.append(blob_path)\n",
        "markets_path_list.remove('gs://scout-storage/markets_final/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFHZeIR_7egA"
      },
      "source": [
        "## Create Dataset & Dataloaders\n",
        "\n",
        "- Create target, date features, and time_idx \n",
        "- define columns \n",
        "- create pytorch dataloader "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWTPV-do5jAR"
      },
      "outputs": [],
      "source": [
        "# data = pd.read_parquet(markets_path_list[1]) #use market 001 b/c it has 2 extra columns ('Inventory (Buildings)', 'Effective Rent % Chg')\n",
        "\n",
        "markets_prefix = \"markets_final/\"\n",
        "market_name = \"market075.parquet\"\n",
        "# market_path = \"gs://\" + markets_prefix + market_name\n",
        "market_path = 'gs://scout-storage/markets_final' +  '/' + market_name\n",
        "\n",
        "data = pd.read_parquet('gs://scout-storage/markets_final/market075.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcEYMfsQ_WQ4",
        "outputId": "37e48e40-b28c-43ab-b269-15d888bf2638"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    Miami\n",
              "1    Miami\n",
              "2    Miami\n",
              "3    Miami\n",
              "4    Miami\n",
              "Name: MARKET_NAME, dtype: object"
            ]
          },
          "execution_count": 33,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['MARKET_NAME'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msNwQljuadOy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Extract features from date-time to capture trend/cyclical behavior as a function of time \n",
        "def add_datepart(df, fldname, drop=True, time=False):\n",
        "  \"Helper function that adds columns relevant to a date.\"\n",
        "  fld = df[fldname]\n",
        "  fld_dtype = fld.dtype\n",
        "  if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
        "      fld_dtype = np.datetime64\n",
        "\n",
        "  if not np.issubdtype(fld_dtype, np.datetime64):\n",
        "      df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
        "  targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
        "  attr = ['Year', 'Month', 'Week', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
        "  if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
        "  for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
        "  df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
        "  if drop: df.drop(fldname, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VfWkmxT8yaA"
      },
      "outputs": [],
      "source": [
        "data['Price_Per_Unit'] = data['Sales Amount']*1000000 // data['PROPERTY_UNITS']\n",
        "# Recalculate NOI \n",
        "#data['NOI'] = (data['EGR'] - (   (data['Rent Actual'] * data['PROPERTY_UNITS']) - ((data['Rent Actual'] * data['PROPERTY_UNITS']) * data['Expenses %'])))\n",
        "#data['Value'] = data['NOI'] / data['Market Cap Rate']\n",
        "#Effective gross revenue - (revenue - (revenue * market expense % ))  = NOI / market cap rate\n",
        "# add time index\n",
        "data[\"time_idx\"] = data[\"Date\"].dt.year * 12 + data[\"Date\"].dt.month\n",
        "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
        "\n",
        "add_datepart(data, 'Date', drop=False)\n",
        "\n",
        "data[['PROPERTY_ZIPCODE','CensusBlockGroup', 'TractCode', 'Month', 'Week', 'Year', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']] = data[['PROPERTY_ZIPCODE','CensusBlockGroup', 'TractCode', 'Month', 'Week', 'Year', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLwFPOVo1e0J",
        "outputId": "e56507c0-6ec9-4e16-fb1e-4efaff4e0b13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    3.381700e+04\n",
              "mean     9.532288e+04\n",
              "std      9.892501e+04\n",
              "min      1.159358e+04\n",
              "25%      3.183883e+04\n",
              "50%      5.550547e+04\n",
              "75%      1.278368e+05\n",
              "max      1.023269e+06\n",
              "Name: OPEX, dtype: float64"
            ]
          },
          "execution_count": 11,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "NaN Features: closing_costs, loan_payoff, LNCF, cost_of_sale, loan_payoff\n",
        "Features we can probably drop: capex_reserves, CBDS (NOI - capex_reserves), UNCF,  (keeping loan proceeds), LECOC, UECOC (calculated incorrectly. CFBDS/equity)\n",
        "\n",
        "Delete anything thats not dynamic (anything below NOI except debt payments)\n",
        "'''\n",
        "data = data.drop(columns = ['closing_costs', 'loan_payoff', 'LNCF', 'cost_of_sale', 'loan_payoff', 'capex_reserves', 'CBDS', 'UNCF', 'LECOC', 'UECOC', 'StateFIPS','CountyFIPS', 'State', 'County'])\n",
        "\n",
        "data['NOI'] = (data['EGR'] - ((data['Rent Actual'] * data['PROPERTY_UNITS']) - ((data['Rent Actual'] * data['PROPERTY_UNITS']) * data['Expenses %']))) \n",
        "data['Value'] = ((data['NOI']  * 12 / data['Market Cap Rate'])// data['PROPERTY_UNITS'])\n",
        "data['cap_rate'] = data['NOI'] // data['Sales Amount']*1000000\n",
        "data['OPEX'] = (data['Rent Actual'] * data['PROPERTY_UNITS']) * data['Expenses %']\n",
        "data['OPEX'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6TDwlV_dSAF"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aul0LQQCDrQK"
      },
      "source": [
        "#### Column definitions pytorch version "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLy-gMNE6ttx"
      },
      "outputs": [],
      "source": [
        "#print(len(pd.read_parquet(markets_path_list[0]).columns.to_list())) #4202 cols\n",
        "#print(len(pd.read_parquet(markets_path_list[1]).columns.to_list())) #4204 cols\n",
        "\n",
        "#difference between two df's column names\n",
        "# Not needed for single market?\n",
        "# list(set(pd.read_parquet(markets_path_list[1]).columns.to_list()) - set(pd.read_parquet(markets_path_list[0]).columns.to_list()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sYgcYm93_W1G"
      },
      "outputs": [],
      "source": [
        "def column_definitions(markets_df):\n",
        "  '''Take in a market_df and output dictionary with column definitions'''\n",
        "\n",
        "  feat_list = markets_df.columns.to_list() \n",
        "  column_definitions = {}\n",
        "  \n",
        "  # Columns that will be dropped in the split_data function\n",
        "  # drop any column with the word 'discontinued' in it *** \n",
        "  # Why are we dropping Lat and Long? Aren't we going to use this along with the geo polygons for location data? \n",
        "  # I moved it from the drop list b/c maybe model can figure out that points that are relatively close to eachother have an effect on values \n",
        "  # drops = ['StateFIPS','CountyFIPS', 'State', 'County']\n",
        "\n",
        "  # Nothing is done with Date. Create new features before running this function. \n",
        "  special_features = ['property_id', 'Date', 'Price_Per_Unit']\n",
        "\n",
        "  time_idx= 'time_idx'\n",
        "   # Value, Price_Per_Unit, Rent Per SqFt Actual, Rent Per SqFt Market\n",
        "  target = 'Value'\n",
        "  group_ids = 'property_id'\n",
        "\n",
        "  time_varying_known_reals = ['Elapsed']\n",
        "  time_varying_known_categoricals = ['Month', 'Week', 'Year', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
        "  \n",
        "  #slice the market data list to keep only the census numerical features \n",
        "  census_features = feat_list[feat_list.index('B00001e1'):feat_list.index('B24080e21')+1]\n",
        "\n",
        "  # static cols from property and market data. \n",
        "  static_categoricals = ['MARKET_NAME', 'SUBMARKET_NAME',\n",
        "                         'COUNTY_NAME', 'PROPERTY_NAME', 'PROPERTY_CITY', \n",
        "                         'PROPERTY_STATE', 'PROPERTY_IMPRATING', 'PROPERTY_LOCRATING',\n",
        "                         'PROPERTY_STUDENTHOUSING', 'PROPERTY_AFFORDABLEHOUSING',\n",
        "                         'PROPERTY_MILITARYHOUSING', 'PROPERTY_AGERESTRICTED', 'PROPERTY_ZIPCODE',\n",
        "                         'CensusBlockGroup', 'TractCode']\n",
        "\n",
        "\n",
        "  \n",
        "  # keep as real valued for now but later on make features of ranges to have cat vars of these feats  \n",
        "  static_reals = census_features + ['PROPERTY_UNITS', 'PROPERTY_SQFT', 'PROPERTY_ACRES', 'PROPERTY_DATECOMPLETED', 'PROPERTY_LATITUDE', 'PROPERTY_LONGITUDE']                    \n",
        "  \n",
        "  not_time_varying_unknown_reals = [target] + special_features + static_categoricals + static_reals + time_varying_known_reals + time_varying_known_categoricals\n",
        "\n",
        "  time_varying_unknown_reals = [x for x in feat_list if x not in not_time_varying_unknown_reals]\n",
        "\n",
        "  \n",
        "  # print('Before')\n",
        "  # print(len(feat_list))\n",
        "  # print(\"\\n\")\n",
        "  # print(len(not_time_varying_unknown_reals))\n",
        "  # print(\"\\n\")\n",
        "  # print(len(time_varying_unknown_reals))\n",
        "\n",
        "  column_definitions['time_idx'] = time_idx\n",
        "  column_definitions['target'] = target\n",
        "  column_definitions['group_ids'] = group_ids\n",
        "  column_definitions['static_categoricals'] = static_categoricals\n",
        "  column_definitions['static_reals'] = static_reals\n",
        "  column_definitions['time_varying_known_categoricals'] = time_varying_known_categoricals\n",
        "  column_definitions['time_varying_known_reals'] = time_varying_known_reals\n",
        "  column_definitions['time_varying_unknown_reals'] = time_varying_unknown_reals\n",
        "  \n",
        "\n",
        "  return column_definitions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YM-2u4TXJbQW"
      },
      "outputs": [],
      "source": [
        "col_def = column_definitions(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ExFefIDHLchO"
      },
      "outputs": [],
      "source": [
        "regions = {\n",
        "    'Alaska-Hawaii': ['Anchorage', 'Honolulu'],\n",
        "    'Central California': ['Central Coast', 'Central Valley'],\n",
        "    'Florida': ['Fort Lauderdale', 'Jacksonville', 'Miami', 'North Central Florida',\n",
        "                'Orlando', 'Pensacola', 'Southwest Florida Coast', 'Tallahassee',\n",
        "                'Tampa', 'West Palm Beach'],\n",
        "    'Mid-Atlantic': ['Baltimore', 'Northern Virginia', 'Richmond-Tidewater', \n",
        "                     'Washington DC - Suburban Maryland'],\n",
        "    'Midwest': ['Chicago - Suburban', 'Chicago - Urban', 'Cincinnati', 'Cleveland-Akron', \n",
        "                'Columbus', 'Dayton', 'Des Moines', 'Detroit', 'Fort Wayne', 'Grand Rapids',\n",
        "                'Indianapolis', 'Kansas City', 'Lafayette', 'Lansing-Ann Arbor', 'Madison',\n",
        "                'Milwaukee', 'Omaha', 'South Bend', 'St Louis', 'Toledo', 'Twin Cities - Suburban',\n",
        "                'Twin Cities - Urban', 'Wichita'],\n",
        "    'Northeast': ['Albany', 'Allentown-Bethlehem', 'Boston', 'Bridgeport-New Haven', \n",
        "                  'Brooklyn', 'Buffalo', 'Harrisburg', 'Long Island', 'Manhattan', \n",
        "                  'New Jersey - Central', 'New Jersey - Northern', 'Philadelphia - Suburban', \n",
        "                  'Philadelphia - Urban', 'Pittsburgh', 'Portland ME', 'Providence', \n",
        "                  'Queens', 'Rochester', 'Syracuse', 'White Plains', 'Worcester - Springfield'],\n",
        "    'Northern California': ['Bay Area - East Bay', 'Bay Area - South Bay', 'Sacramento', 'San Francisco'],\n",
        "    'Pacific Northwest': ['Eugene', 'Portland', 'Richland-Kennewick-Pasco', 'Seattle', 'Spokane', 'Tacoma'],\n",
        "    'South': ['Baton Rouge', 'Birmingham', 'Chattanooga', 'Huntsville', 'Jackson', 'Knoxville', \n",
        "              'Lafayette - Lake Charles', 'Lexington Fayette', 'Little Rock', 'Louisville', \n",
        "              'Memphis', 'Mobile', 'Nashville', 'New Orleans'],\n",
        "    'Southeast': ['Asheville', 'Atlanta - Suburban', 'Atlanta - Urban', 'Augusta GA', 'Charleston', \n",
        "                  'Charlotte', 'Columbia', 'Columbus GA', 'Greenville', 'Macon', 'Raleigh - Durham', \n",
        "                  'Savannah - Hilton Head', 'Wilmington', 'Winston-Salem-Greensboro'],\n",
        "    'Southern California': ['Inland Empire', 'Los Angeles - Eastern County',\n",
        "                            'Los Angeles - Metro', 'Orange County', 'San Diego',\n",
        "                            'San Fernando Valley-Ventura County'],\n",
        "    'Southwest': ['Amarillo', 'Austin', 'Central East Texas', 'Corpus Christi', 'Dallas - North', \n",
        "                  'Dallas - Suburban', 'El Paso', 'Fort Worth', 'Houston - East', 'Houston - West', \n",
        "                  'Lubbock', 'McAllen', 'Midland - Odessa', 'Oklahoma City', 'San Antonio', 'Tulsa'],\n",
        "    'Western': ['Albuquerque', 'Boise', 'Colorado Springs', 'Denver', 'Las Vegas',\n",
        "                'Phoenix', 'Reno', 'Salt Lake City', 'Tucson'],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XhDZ31GilMxl"
      },
      "outputs": [],
      "source": [
        "#fill all columns with real values with 0's for the nans \n",
        "#adding in either strings as nan to the cols with numbers \n",
        "# or adding number 0 to the column with strings. Must be uniform datatype\n",
        "data[col_def['time_varying_unknown_reals'] + col_def['static_reals'] + [col_def['target']]] = data[col_def['time_varying_unknown_reals'] + col_def['static_reals'] + [col_def['target']]].fillna(0)\n",
        "data[col_def['static_categoricals'] + col_def['time_varying_known_categoricals']] = data[col_def['static_categoricals'] + col_def['time_varying_known_categoricals']].fillna('NaN') #np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASV-7g0YjrD_"
      },
      "outputs": [],
      "source": [
        "# data.groupby('property_id', observed=True).head()\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA6ci-IsheUq"
      },
      "outputs": [],
      "source": [
        "for col in data[col_def['static_categoricals']]: \n",
        "  # check where nan's exist\n",
        "  print('Does null exist in {}? : {}'.format(col, data[col].isnull().values.any()))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4GLbHmFSMP6j",
        "outputId": "6db75bf8-f88a-40db-9a31-0e7638e79187"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'property_id'"
            ]
          },
          "execution_count": 28,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "col_def['group_ids']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjEXMkqyrsoF"
      },
      "source": [
        "# Create Datasets & Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "w1ayV_omr3ah",
        "outputId": "ec3dc8bd-0985-4684-d84c-ca0923d19b6f"
      },
      "outputs": [],
      "source": [
        "#need to change these. prob to 36 for prediction & then for encoder. max_econder_length = 72 (2010 - 1016)\n",
        "max_prediction_length = 6\n",
        "max_encoder_length = 3914\n",
        "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
        "\n",
        "training = TimeSeriesDataSet(\n",
        "    data[lambda x: x.time_idx <= training_cutoff],\n",
        "    time_idx = col_def['time_idx'],\n",
        "    target = col_def['target'], #Value\n",
        "    group_ids = col_def['group_ids'], \n",
        "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
        "    max_encoder_length=max_encoder_length,\n",
        "    min_prediction_length=1,\n",
        "    max_prediction_length=max_prediction_length,\n",
        "    static_categoricals = col_def['static_categoricals'], \n",
        "    static_reals = col_def['static_reals'], \n",
        "    time_varying_known_categoricals = col_def['time_varying_known_categoricals'], \n",
        "    variable_groups = {},  # group of categorical variables can be treated as one variable\n",
        "    time_varying_known_reals = col_def['time_varying_known_reals'], \n",
        "    time_varying_unknown_categoricals = [], \n",
        "    time_varying_unknown_reals = col_def['time_varying_unknown_reals'],\n",
        "    target_normalizer=GroupNormalizer(\n",
        "        groups = col_def['group_ids'], coerce_positive=1.0\n",
        "    ),  # use softplus with beta=1.0 and normalize by group\n",
        "    add_relative_time_idx = True,\n",
        "    add_target_scales = True,\n",
        "    add_encoder_length = True,\n",
        ")\n",
        "\n",
        "# create validation set (predict=True) which means to predict the last max_prediction_length points in time for each series\n",
        "validation = TimeSeriesDataSet.from_dataset(training, data, predict = True, stop_randomization = True)\n",
        "\n",
        "# create dataloaders for model\n",
        "batch_size = 128  # set this between 32 to 128\n",
        "train_dataloader = training.to_dataloader(train = True, batch_size = batch_size, num_workers = 0)\n",
        "val_dataloader = validation.to_dataloader(train = False, batch_size = batch_size * 10, num_workers = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2kourgwLMvd"
      },
      "source": [
        "# Create Baseline Model\n",
        "\n",
        "Evaluating a baseline model that predicts the next 36 months by simply repeating the last observed target gives us a simle benchmark that we want to outperform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtfOAFRELYKd"
      },
      "outputs": [],
      "source": [
        "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
        "actuals = torch.cat([y for x, y in iter(val_dataloader)])\n",
        "baseline_predictions = Baseline().predict(val_dataloader)\n",
        "(actuals - baseline_predictions).abs().mean().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCU1S-uldeKR"
      },
      "source": [
        "# Train the TFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0e5jYuadl3c"
      },
      "source": [
        "## Find the Optimal Learning Rate\n",
        "- This probably will be a bit off — lr_find() doesn't always return the best result but it's good practice to get it done. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlG-nv_9dgEe"
      },
      "outputs": [],
      "source": [
        "# configure network and trainer\n",
        "pl.seed_everything(42)\n",
        "trainer = pl.Trainer(\n",
        "    gpus=0,\n",
        "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
        "    # of the gradient for recurrent neural networks\n",
        "    gradient_clip_val=0.1,\n",
        ")\n",
        "\n",
        "\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    # not meaningful for finding the learning rate but otherwise very important\n",
        "    learning_rate=0.03,\n",
        "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
        "    # number of attention heads. Set to up to 4 for large datasets\n",
        "    attention_head_size=1,\n",
        "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
        "    hidden_continuous_size=8,  # set to <= hidden_size\n",
        "    output_size=7,  # 7 quantiles by default\n",
        "    loss=QuantileLoss(),\n",
        "    # reduce learning rate if no improvement in validation loss after x epochs\n",
        "    reduce_on_plateau_patience=4,\n",
        ")\n",
        "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4-8fWziebQ6"
      },
      "outputs": [],
      "source": [
        "# find optimal learning rate\n",
        "res = trainer.tuner.lr_find(\n",
        "    tft,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloaders=val_dataloader,\n",
        "    max_lr=10.0,\n",
        "    min_lr=1e-6,\n",
        ")\n",
        "\n",
        "print(f\"suggested learning rate: {res.suggestion()}\")\n",
        "fig = res.plot(show=True, suggest=True)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nEXPZzdej99"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iMp0F65elRo"
      },
      "outputs": [],
      "source": [
        "# configure network and trainer\n",
        "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
        "lr_logger = LearningRateMonitor()  # log the learning rate\n",
        "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=30,\n",
        "    gpus=0,\n",
        "    weights_summary=\"top\",\n",
        "    gradient_clip_val=0.1,\n",
        "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
        "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
        "    callbacks=[lr_logger, early_stop_callback],\n",
        "    logger=logger,\n",
        ")\n",
        "\n",
        "\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    learning_rate=0.03,\n",
        "    hidden_size=16,\n",
        "    attention_head_size=1,\n",
        "    dropout=0.1,\n",
        "    hidden_continuous_size=8,\n",
        "    output_size=7,  # 7 quantiles by default\n",
        "    loss=QuantileLoss(),\n",
        "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
        "    reduce_on_plateau_patience=4,\n",
        ")\n",
        "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8a_yV5Zemvz"
      },
      "outputs": [],
      "source": [
        "# fit network\n",
        "trainer.fit(\n",
        "    tft,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloaders=val_dataloader,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Vc59MnmN0at-"
      ],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "Scout_TFT_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
